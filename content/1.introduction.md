## Introduction
{:#introduction}

Decentralized data ecosystems are reshaping how data is stored, accessed, and governed.
Rather than centralizing data around consumers, these ecosystems center data around producers,
empowering individuals and organizations to maintain ownership and control.
This shift brings clear benefits:
1. it facilitates collaboration across organizational boundaries despite differing data models and objectives,
2. it reduces redundant replication through fine-grained access policies, and
3. it enables diverse applications to reuse shared data sources.
These properties are particularly beneficial in the emerging age of agentic AI -
autonomous systems capable of acting on behalf of individuals.

However, agentic systems -
whether powered by language models or more traditional data interaction models (e.g. query engines) -
require a precise understanding of both the semantics and the consequences of the actions they perform.
When these systems interact with decentralized data sources, they need to know:
1. What does it mean to write data here?
2. What changes will this action trigger?
3. Which interfaces reflect the desired state of the world after such an operation?
In practice, today's interfaces rarely answer these questions fully.
Descriptions are often partial, human-readable, and inconsistently maintained.
As a result, 
autonomous agents operate with brittle assumptions and incomplete models of the systems they interact with.

One key insight from previous research is that symmetric interfaces - 
where writes and reads have the same granularity, and the same resources is used for both reading and writing -
[fail to support the nuanced policy definitions required in these ecosystems](cite:cites whatsinapod).
Asymmetric interfaces,
in which write operations and read queries are decoupled, are a more promising fit.
But they introduce a new challenge:
for autonomous agents (e.g. query engines and agentic systems), write semantics become opaque.
A write to one endpoint can have various consequences on another.
This creates a semantic gap that prevents agents from reliably reasoning over their own actions.

At the same time, each datastore in a decentralized environment could be considered a polyglot system.
The semantic web community has a long history exposing data, and has learned that [the perfect interface does not exist](cite:cites tpf, hartig2017formal).
The perfect interface is use-case dependent and depends at any time on the current query type and load. 
Decentralized data ecosystems center data around data producers, which allow consumption of their data through various use cases.
Since no single interface can satisfy the varying performance, policy, and usability constraints of all use cases, a data consumer can decide to expose their data through multiple, partially overlapping interfaces, effectively creating a [polyglot system](cite:cites khine2019review).
Moreover, in dynamic environments,
interfaces themselves may change at runtime to optimize for current query type and load - 
mirroring the elasticity of cloud infrastructure.

To manage this complexity, we argue for the need for Interface Data Descriptions (IDDs) -
machine-readable, algebraic descriptions that express how interfaces relate to underlying datasets and to each other.
Rather than relying solely on verbal descriptions of API behavior,
IDD formalizes data interfaces as data transformations linking writes to reads.
This perspective allows us to describe not just what data is accessible via an interface,
but how the data is derived or modified, and what guarantees are associated with those transformations.
By describing interfaces over a dataset in space $D$ using endofunctions (type $D -> D%),
we can enable autonomous agents to fully grasp the consequences of performing actions.

This paper presents a vision for a future in which decentralized data ecosystems are equipped with rich,
composable, and formal interface semantics.
Our goal is to stimulate discussion on how such descriptions can support autonomous operation,
improve agent reliability, and pave the way for more interoperable and adaptive knowledge infrastructures.
While the proposed direction remains conceptual,
we offer initial ideas and mappings that illustrate how this approach could be realized in practice.
