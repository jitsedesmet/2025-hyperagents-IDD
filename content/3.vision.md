## Vision
{:#vision}

Our vision is to bridge the gap between reads and writes in decentralized data ecosystems by describing their relationship through algebraic flows.
These flows formalize how read interfaces derive their contents from past write operations,
enabling a machine-readable and composable account of data movement and transformation.

To enable this, we make several principled design choices.

First, we acknowledge that data evolves along a temporal axis -
changes occur over time -
but not all interfaces expose or require access to this history.
Many read interfaces return the current state of the data,
abstracting away its temporal provenance.
Rather than imposing a specific model of time or event logging,
we adopt a flexible dual-space approach:

1. **State space**: which describes data as it exists at any given moment. 
2. **Event space**: which models the individual write actions that cause transitions between states.

Second, we emphasize that the interface description is decoupled from the implementation.
That is, the algebraic description models the semantics of the data exposed through an interface -
not the internal mechanics of the underlying system.
For example, an RDF-based description may describe a JSON API backed by a relational database.
The description remains useful, even if the actual interface does not natively speak RDF.
Despite this abstraction, the algebraic description is sufficient to generate a server implementation, illustrating the expressive power of the model.

While our examples are grounded in [SPARQL algebra](cite:cites spec:sparqllang) over [RDF](cite:cites spec:rdf) -
due to RDF's strengths in representing linked,
web-native data using IRIs - the broader approach is algebra-agnostic.
Any algebra that can express data transformations in a way that preserves semantic meaning and exposes IRIs as first-class citizens could be used.
The choice of RDF is especially compelling in this context,
as IRIs allow for rich linking across organizational boundaries, enabling decentralized, federated knowledge infrastructures.

### State Space

At its core, describing a read interface in the state space is relatively straightforward.
Each interface can be seen as a function that maps one or more datasets to a (possibly filtered or reshaped) view over themselves.
Formally, this means the mapping must be an endofunction: $$F:D \rightarrow D$$.

In the context of RDF, such a function can be represented by a SPARQL CONSTRUCT query.
Given RDF source datasets, a read interface is defined by applying a construct query to produce a derived dataset - this result is what the interface exposes to clients.

To fully describe the interface, we must also define:
1. the interaction method - how the interface is accessed (e.g. HTTP GET, SPARQL 1.1, GraphQL), and 
2. the HTTP resource - the concrete endpoint at which the data is made available.

These components capture the semantics of how data is retrieved without tying the description to a specific backend implementation.
This design embraces heterogeneity, allowing the same description model to span various technologies,
and makes it possible to reason about data behavior across evolving protocols.
This is akin to how systems like Comunica handle interface heterogeneity by abstracting interaction protocols.

Through the creation of parameterized interfaces, a single algebraic description can yield infinitely many concrete interfaces through the use of variables or query parameters.

In [](#state-def-intro) shows an example definition of a mapping over a single RDF database, while
[](#state-def-multiple-interfaces) illustrates how one description can define a family of interfaces parameterized by HTTP query parameters. 

### Event space

In contrast to state space, where data is modeled as static snapshots,
event space describes transient operations that represent state transitions.
Write interfaces inherently operate within event space:
they define actions that, when executed, could update a persistent state, but the event itself is ephemeral -
a message, not a mutation.

In this model, operations within event space follow the same algebraic form as those in state space:
they are still defined by mappings between datasets.
However, the semantics shift - these datasets represent transient event messages, not durable states.
An event-space interface does not write to disk by default.
Instead, it emits an anonymous graph internal to the system, which is then processed or enriched by other mappings.

As in the state space, a write interface is defined by:
1. An interaction method (e.g. HTTP POST), 
2. An HTTP resource, and 
3. A mapping that transforms the incoming RDF payload into a new event-space graph.

In other words, each write interface defines how a user-supplied RDF document is transformed into an internal graph using a CONSTRUCT-style operation.
This transformation may bind identifiers (e.g. through UUIDs) and enrich or restructure the data.
See [](#write-init) for an example definition.

Event-space mappings can be composed just like state-space mappings.
A read interface over an event-space graph (see [](#event-mapping)) can, for instance,
filter or extract views over the transient event data before it is committed to persistent state.

It is important to distinguish between interface URIs (which are the URIs used by the API) and dataset URIs (which re the URIs used to name datasets).

### Changing Space

The transition from event space to state space is made possible by leveraging set operations over RDF datasets.
Since RDF inherently represents data as sets of triples,
we can describe state changes using set operations such as union and difference.
This allows us to model the accumulation, modification, or removal of event-sourced data into a durable state.

A write interface emits an event graph,
and stateful storage interfaces can define their contents by accumulating or transforming these event graphs.
[](#event-to-state-space) shows an example where the state is defined by applying a union over creation events and a transformation over patch events that combines both union and difference operations.

The inverse transition - from state space back to event space - is generally not possible.
This is because state alone does not carry temporal intent:
there is no information in the state that tells the system when or why an event should be emitted.
However, event-space mappings may draw from state-space resources,
for example to include current state in a write or to compute a delta against the latest values.

Working with blank nodes in this context introduces complications.
Because blank nodes are existential and not globally referencable,
set operations like difference become unreliable or ambiguous.
We therefore recommend avoiding blank nodes in stateful datasets.
Alternatively, mappings may skolemize blank nodes (i.e. assign them stable IRIs),
though this technique does not fully resolve all the subtleties of identity and merging.

Finally, carefully defined write interfaces -
and their corresponding event-to-state mappings -
can serve as the system's consistency boundaries.
These interfaces act like commit points in a transactional system:
they define how state may evolve and enforce structural constraints.

### Consumption of algebraic definitions

With a full description of both read and write interfaces,
we are now equipped to describe how data flows through a system -
how read interfaces derive their data from upstream writes,
and how writes propagate through event and state space into persistent views.

Given such a description (e.g. as shown in []()),
an automated agent can scope itself to the relevant datasets (e.g. `<database>`),
and infer how to interact with the system both to read and to modify data.

These algebraic descriptions enable the agent to:
1. Discover what interfaces provide views over the dataset, and how to query them, and 
2. infer how modifications to those views can be enacted,
by tracing flows back to the corresponding write interfaces.

This algebraic interface description provides more than just interaction details.
It conveys:
1. What endpoints can be used to read or modify a dataset. 
2. How these endpoints can be used (interaction methods, required shapes). 
3. Why certain interactions affect the state (tracing data provenance). 
4. What consequences a modification will have (based on how state is computed from events).

In this way, our model enables agents to reason not just about what data exists,
but how it came to be, how it can be changed, and what such changes would entail.


#### Example

Consider the dataset modeled in our earlier examples.
There are two write interfaces: `<post-owner>` and `<patch-owner>`.
Each describes the shape of its expected input and how it transforms that input into internal event-space resources.

The `<post-owner>` interface expects input of the following shape:
```shex
<> {
    ex:name xsd:string;
    ex:pets {
        foaf:name xsd:string;
        ex:age    xsd:int;
    }*
}
```
Its mapping transforms this input into a new graph that can be expressed through its relation with the input:
```
eval()_1 a ex:owner
eval()_1 ex:name xsd:string_1
eval()_1 ex:pets* eval()_2
eval()_2 a ex:pet
eval()_2 ex:name xsd:string_2
eval()_2 ex:age xsd:int_1
```

The `<patch-owner>` interface, similarly, transforms a patch document:
```shex
<> {
    ex:name xsd:string;
}
```
into
```
eval()_3 a ex:owner-patch
eval()_3 ex:name xsd:string_3
eval()_3 ex:resource ?patch
```

These event-space outputs are then combined via set operations (e.g., union, difference)
to define the durable `<database>` state.
From the mappings, we can infer the resulting shape of this dataset:
```
eval()_1 a ex:owner
eval()_1 ex:name xsd:string_1 | xsd:string_3
eval()_1 ex:pets* eval_2()
eval()_2 a ex:pet
eval()_2 ex:name xsd:string_2
eval()_2 ex:age xsd:int_1
```

Read interfaces such as `<get-owners>` and `<get-owner>` provide access to this derived state.
Suppose an agent wants to change the name of a specific owner,
e.g. deleting triple `<some-owner> ex:name "some name"` and inserting triple `<some-owner> ex:name "new name"`.
The agent traces the existing triple to its origin:
it is derived from either xsd:string_1 or xsd:string_3 in the event mappings.
It identifies that the only interface capable of replacing `ex:name` for an existing owner is `<patch-owner>`,
which takes a target resource and a new name as input.
Thus, the agent deduces that to perform the update, it must submit:
`ex:owners/{uuid} ex:name "new name"` to the `<patch-owner>` endpoint.


A vital part of why we use algebra is the fact that it can be optimized. The flow does not need to be xecuted as a whole.
Each read interface could optimize itself in relation to the write interfaces or durable stores.