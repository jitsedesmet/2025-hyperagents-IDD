## Vision
{:#vision}

Our vision is to bridge the gap between reads and writes in decentralized data ecosystems by describing their relationship through algebraic flows.
These flows formalize how read interfaces derive their contents from past write operations,
enabling a complete and machine-understandable description of the data processes.

To enable this, we make several principled design choices.

First, we acknowledge that data evolves along a temporal axis -
changes occur over time -
but not all interfaces expose or require access to this history.
Many read interfaces return the current state of the data,
abstracting away its temporal provenance.
Rather than imposing a specific model of time or event logging,
we adopt a flexible dual-space approach:

1. **State space**: which describes durable data as it exists at any given moment. 
2. **Event space**: which models the individual write events that cause transitions between states.

Second, we emphasize that the interface description is decoupled from the implementation.
That is, the algebraic description models the semantics of the data exposed through an interface -
not the internal mechanics of the underlying system.
For example, an RDF-based description may describe a JSON API backed by a relational database.
The description remains useful, even if the actual interface does not natively speak RDF.
Despite this abstraction, the algebraic description is sufficient to generate a server implementation, illustrating the expressive power of the model.

While our examples are grounded in [SPARQL algebra](cite:cites spec:sparqllang) over [RDF](cite:cites spec:rdf) -
due to RDF's strengths in representing linked,
web-native data using IRIs - the broader approach is algebra-agnostic.
Any algebra that can express data transformations in a way that preserves semantic meaning and exposes IRIs as first-class citizens could be used.
The choice of RDF is especially compelling in this context,
as IRIs allow for rich linking across organizational boundaries, enabling decentralized knowledge infrastructures.

### State Space

At its core, describing a read interface in the state space is relatively straightforward.
Each interface can be seen as a function that maps one or more datasets to a (possibly filtered or reshaped) view over themselves.
Formally, this means the mapping must be an endofunction: $$F:D \rightarrow D$$.

In the context of RDF, such a function can be represented by a SPARQL CONSTRUCT query.
Given RDF source datasets, a read interface is defined by applying a construct query to produce a derived dataset - this result is what the interface exposes to clients.

To fully describe the interface, we must also define:
1. the interaction method - how the interface is accessed (e.g. HTTP GET, SPARQL 1.1, GraphQL), and 
2. the HTTP resource - the concrete endpoint at which the data is made available.

These components capture the semantics of how data is retrieved without tying the description to a specific backend implementation.
This design embraces heterogeneity, allowing the same description model to span various technologies,
and makes it possible to reason about data behavior across evolving protocols.
This is akin to how systems like [Comunica](cite:cites comunica) handle interface heterogeneity by abstracting interaction protocols.

Through the creation of parameterized interfaces, a single algebraic description can yield infinitely many concrete interfaces through the use of variables or query parameters.

[](#5.get-owners) shows an example definition of a mapping over a single RDF database, while
[](#6.get-owner) illustrates how one description can define a family of interfaces parameterized by HTTP query parameters.
It is important to distinguish between interface URIs (which are the URIs used by the API) and dataset URIs (which are the URIs used to name datasets).

### Event space

In contrast to state space, where data is modeled as static snapshots,
event space describes transient operations that represent state transitions.
Write interfaces inherently operate within event space:
they define actions that, when executed, could update a persistent state, but the event itself is ephemeral -
a message, not a mutation.

In this model, operations within event space follow the same algebraic form as those in state space:
they are still defined by mappings between datasets.
However, the semantics shift - these datasets represent transient event messages, not durable states.
An event-space interface does not write to disk by default.
Instead, it emits an anonymous graph internal to the system, which is then processed or enriched by other mappings.

As in the state space, a write interface is defined by:
1. An interaction method (e.g. HTTP POST), 
2. An HTTP resource (e.g. URI/ URI template), and 
3. A mapping that transforms the incoming RDF payload into a new event-space graph.

In other words, each write interface defines how a user-supplied RDF document is transformed into an internal graph using a CONSTRUCT-style operation.
This transformation may bind identifiers (e.g. through UUIDs) and enrich or restructure the data.
[](#1.post-owner) and [](#2.patch-owner) show two example definitions of write interfaces.

Event-space mappings can be composed just like state-space mappings.
An event based read interface can for example be created by deriving itself based on another event-space resource.
[](#7.websocket) shows an example read interface exposed through websockets.

### Changing Space

The transition from event space to state space is made possible by leveraging set operations over RDF datasets.
Since RDF inherently represents data as sets of triples,
we can describe state changes using set operations such as union and difference.
This allows us to model the accumulation, modification, or removal of event-sourced data into a durable state.

A write interface emits an event graph,
and stateful storage interfaces can define their contents by accumulating or transforming these event graphs.
[](#4.database-def) shows an example where the state is defined by applying a union over creation events and
two transformations over patch events that combine both union and difference operations.

The inverse transition - from state space back to event space - is generally not possible.
This is because state alone does not carry temporal intent:
there is no information in the state that tells the system when or why an event should be emitted.
However, event-space mappings may draw from state-space resources,
for example to include current state in a write or to compute a delta against the latest values.

Working with blank nodes in this context introduces complications.
Because blank nodes are existential and not globally referencable,
set operations like difference become unreliable or ambiguous.
We therefore recommend avoiding blank nodes in stateful durable datasets.
Alternatively, mappings may skolemize blank nodes (i.e. assign them stable IRIs),
though this technique does not fully resolve all the subtleties of identity and merging.

Finally, carefully defined write interfaces -
and their corresponding event-to-state mappings -
can serve as the system's consistency boundaries.
These interfaces act like commit points in a transactional system:
they define how state may evolve and enforce structural constraints.
[](#3.change-owner) shows such an example consistent operation.

### Consumption of algebraic definitions

With a full description of both read and write interfaces,
we are now equipped to describe how data flows through a system -
how read interfaces derive their data from upstream writes,
and how writes propagate through event and state space into persistent views.

Given such a description (e.g. as shown in [](#annex)),
an automated agent can scope itself to the relevant datasets (e.g. `<database>`),
and infer how to interact with the system to both read and modify data.

These algebraic descriptions enable the agent to:
1. Discover what interfaces provide views over the dataset, and how to query them, and 
2. infer how modifications to those views can be enacted,
by tracing flows back to the corresponding write interfaces (see [](#flow-example)).
3. Additionally, the use of algebra allows us to optimize the construct-flows into a single big construct query over
the write interfaces and the current version of persisted stores. 

This algebraic interface description provides more than just interaction details.
It conveys:
1. What endpoints can be used to read or modify a dataset. 
2. How these endpoints can be used (interaction methods, required shapes). 
3. Why certain interactions affect the state (tracing data provenance). 
4. What consequences a modification will have (based on how state is computed from events).

In this way, our model enables agents to reason not just about what data exists,
but how it came to be, how it can be changed, and what such changes would entail.
